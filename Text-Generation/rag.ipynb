{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged_pdf_file is gemeration_dataset.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfMerger\n",
    "\n",
    "pdf_files= ['Artificial Intelligence For Dummies.pdf','AI_12_probability.pdf','AI_13_Bayes_nets.pdf','AI_14_Markov_models.pdf','AI_1_Introduction.pdf'\n",
    "            ,'AI_2_Agents.pdf','AI_3_Search.pdf','AI_6_CSP.pdf','AI_7_PL.pdf','AI_8_FOL.pdf','AI_book.pdf']\n",
    "\n",
    "merger = PdfMerger()\n",
    "for pdf in pdf_files:\n",
    "    merger.append(pdf)\n",
    "\n",
    "output_file = \"gemeration_dataset.pdf\"\n",
    "merger.write(output_file)\n",
    "merger.close()\n",
    "\n",
    "print(f\"merged_pdf_file is {output_file}\")\n",
    "\n",
    "import fitz \n",
    "from tqdm.auto import tqdm \n",
    "def text_formatter(text: str) -> str: \n",
    "    \"\"\"Performs minor formatting on text.\"\"\"\n",
    "    cleaned_text = text.replace(\"\\n\", \" \").strip()\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages_and_texts = [] \n",
    "    for page_number, page in tqdm(enumerate(doc)):\n",
    "        text = page.get_text()\n",
    "        text = text_formatter(text=text)\n",
    "        pages_and_texts.append({\"page_number\": page_number - 41,\n",
    "                                \"page_char_count\": len(text),\n",
    "                                \"page_word_count\": len(text.split(\" \")),\n",
    "                                \"page_setence_count_raw\": len(text.split(\". \")),\n",
    "                                \"page_token_count\": len(text) / 4, \n",
    "                                \"text\": text})\n",
    "    return pages_and_texts\n",
    "\n",
    "pages_and_texts = open_and_read_pdf(pdf_path='gemeration_dataset.pdf')\n",
    "pages_and_texts[:2]\n",
    "import random\n",
    "\n",
    "random.sample(pages_and_texts, k=3)\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.head()\n",
    "df.describe().round(2)\n",
    "\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "doc = nlp(\"This is a sentence. This another sentence. I like elephants.\")\n",
    "assert len(list(doc.sents)) == 3\n",
    "\n",
    "list(doc.sents)\n",
    "pages_and_texts[600]\n",
    "\n",
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n",
    "\n",
    "    item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
    "\n",
    "    item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])\n",
    "random.sample(pages_and_texts, k=1)\n",
    "\n",
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.describe().round(2)\n",
    "num_sentence_chunk_size = 10\n",
    "\n",
    "def split_list(input_list: list[str],\n",
    "               slice_size: int=num_sentence_chunk_size) -> list[list[str]]:\n",
    "    return [input_list[i:i+slice_size] for i in range(0, len(input_list), slice_size)]\n",
    "\n",
    "test_list = list(range(25))\n",
    "split_list(test_list)\n",
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n",
    "                                         slice_size=num_sentence_chunk_size)\n",
    "    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])\n",
    "random.sample(pages_and_texts, k=1)\n",
    "import re\n",
    "\n",
    "pages_and_chunks = []\n",
    "for item in tqdm(pages_and_texts): \n",
    "    for sentence_chunk in item[\"sentence_chunks\"]: \n",
    "        chunk_dict = {}\n",
    "        chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
    "\n",
    "        joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
    "        joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk)\n",
    "\n",
    "        chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "\n",
    "        chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "        chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
    "        chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4 \n",
    "\n",
    "        pages_and_chunks.append(chunk_dict) \n",
    "\n",
    "len(pages_and_chunks)\n",
    "random.sample(pages_and_chunks, k=1)\n",
    "df = pd.DataFrame(pages_and_chunks)\n",
    "df.describe().round(2)\n",
    "df.head()\n",
    "\n",
    "min_token_length = 30\n",
    "for row in df[df[\"chunk_token_count\"] <= min_token_length].sample(5).iterrows():\n",
    "    print(f'Chunk token count: {row[1][\"chunk_token_count\"]} | Text: {row[1][\"sentence_chunk\"]}')\n",
    "\n",
    "pages_and_chunks_over_min_token_len = df[df[\"chunk_token_count\"] > min_token_length].to_dict(orient=\"records\")\n",
    "pages_and_chunks_over_min_token_len[:2]\n",
    "random.sample(pages_and_chunks_over_min_token_len, k=1)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\",\n",
    "                                      device=\"cpu\")\n",
    "\n",
    "sentences = [\"The Sentence Transformer library provides an easy way to create embeddings.\",\n",
    "             \"Sentences can be embedded one by one or in a list.\",\n",
    "             \"I like horses!\"]\n",
    "\n",
    "embeddings = embedding_model.encode(sentences)\n",
    "embeddings_dict = dict(zip(sentences, embeddings))\n",
    "\n",
    "for sentence, embedding in embeddings_dict.items():\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Embedding: {embedding}\")\n",
    "    print(\"\")\n",
    "%%time\n",
    "\n",
    "embedding_model.to(\"cuda\")\n",
    "\n",
    "for item in tqdm(pages_and_chunks_over_min_token_len):\n",
    "    item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])\n",
    "text_chunks = [item[\"sentence_chunk\"] for item in pages_and_chunks_over_min_token_len]\n",
    "\n",
    "%%time\n",
    "\n",
    "text_chunk_embeddings = embedding_model.encode(text_chunks,\n",
    "                                               batch_size=32, \n",
    "                                               convert_to_tensor=True) \n",
    "\n",
    "text_chunk_embeddings\n",
    "text_chunks_and_embeddings_df = pd.DataFrame(pages_and_chunks_over_min_token_len)\n",
    "embeddings_df_save_path = \"text_chunks_and_embeddings_df.csv\"\n",
    "text_chunks_and_embeddings_df.to_csv(embeddings_df_save_path, index=False)\n",
    "import pandas as pd\n",
    "text_chunks_and_embedding_df_load = pd.read_csv(embeddings_df_save_path)\n",
    "text_chunks_and_embedding_df_load.head()\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "text_chunks_and_embedding_df = pd.read_csv(\"text_chunks_and_embeddings_df.csv\")\n",
    "\n",
    "text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "\n",
    "pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
    "\n",
    "embeddings = torch.tensor(np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()), dtype=torch.float32).to(device)\n",
    "embeddings.shape\n",
    "text_chunks_and_embedding_df.head()\n",
    "embeddings[0]\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", \n",
    "                                      device=device) query = \"Machine Learning\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "from time import perf_counter as timer\n",
    "\n",
    "start_time = timer()\n",
    "dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]\n",
    "end_time = timer()\n",
    "\n",
    "print(f\"Time take to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
    "\n",
    "top_results_dot_product = torch.topk(dot_scores, k=5)\n",
    "top_results_dot_product\n",
    "larger_embeddings = torch.randn(100*embeddings.shape[0], 768).to(device)\n",
    "print(f\"Embeddings shape: {larger_embeddings.shape}\")\n",
    "\n",
    "start_time = timer()\n",
    "dot_scores = util.dot_score(a=query_embedding, b=larger_embeddings)[0]\n",
    "end_time = timer()\n",
    "\n",
    "print(f\"Time take to get scores on {len(larger_embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
    "import textwrap\n",
    "\n",
    "def print_wrapped(text, wrap_length=80):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Results:\")\n",
    "for score, idx in zip(top_results_dot_product[0], top_results_dot_product[1]):\n",
    "    print(f\"Score: {score:.4f}\")\n",
    "    print(\"Text:\")\n",
    "    print_wrapped(pages_and_chunks[idx][\"sentence_chunk\"])\n",
    "    print(f\"Page number: {pages_and_chunks[idx]['page_number']}\")\n",
    "    print(\"\\n\")\n",
    "import fitz\n",
    "\n",
    "pdf_path = \"gemeration_dataset.pdf\" \n",
    "doc = fitz.open(pdf_path)\n",
    "page = doc.load_page(5 + 41) \n",
    "\n",
    "img = page.get_pixmap(dpi=300)\n",
    "\n",
    "doc.close()\n",
    "\n",
    "img_array = np.frombuffer(img.samples_mv, \n",
    "                          dtype=np.uint8).reshape((img.h, img.w, img.n))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(13, 10))\n",
    "plt.imshow(img_array)\n",
    "plt.title(f\"Query: '{query}' | Most relevant page:\")\n",
    "plt.axis('off') \n",
    "plt.show()\n",
    "import torch\n",
    "\n",
    "def dot_product(vector1, vector2):\n",
    "    return torch.dot(vector1, vector2)\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = torch.dot(vector1, vector2)\n",
    "\n",
    "    norm_vector1 = torch.sqrt(torch.sum(vector1**2))\n",
    "    norm_vector2 = torch.sqrt(torch.sum(vector2**2))\n",
    "\n",
    "    return dot_product / (norm_vector1 * norm_vector2)\n",
    "\n",
    "vector1 = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "vector2 = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "vector3 = torch.tensor([4, 5, 6], dtype=torch.float32)\n",
    "vector4 = torch.tensor([-1, -2, -3], dtype=torch.float32)\n",
    "\n",
    "print(\"Dot product between vector1 and vector2:\", dot_product(vector1, vector2))\n",
    "print(\"Dot product between vector1 and vector3:\", dot_product(vector1, vector3))\n",
    "print(\"Dot product between vector1 and vector4:\", dot_product(vector1, vector4))\n",
    "\n",
    "print(\"Cosine similarity between vector1 and vector2:\", cosine_similarity(vector1, vector2))\n",
    "print(\"Cosine similarity between vector1 and vector3:\", cosine_similarity(vector1, vector3))\n",
    "print(\"Cosine similarity between vector1 and vector4:\", cosine_similarity(vector1, vector4))\n",
    "def retrieve_relevant_resources(query: str,\n",
    "                                embeddings: torch.tensor,\n",
    "                                model: SentenceTransformer=embedding_model,\n",
    "                                n_resources_to_return: int=5,\n",
    "                                print_time: bool=True):\n",
    "    \"\"\"\n",
    "    Embeds a query with model and returns top k scores and indices from embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    query_embedding = model.encode(query, \n",
    "                                   convert_to_tensor=True) \n",
    "\n",
    "    start_time = timer()\n",
    "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
    "    end_time = timer()\n",
    "\n",
    "    if print_time:\n",
    "        print(f\"[INFO] Time taken to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
    "\n",
    "    scores, indices = torch.topk(input=dot_scores, \n",
    "                                 k=n_resources_to_return)\n",
    "\n",
    "    return scores, indices\n",
    "\n",
    "def print_top_results_and_scores(query: str,\n",
    "                                 embeddings: torch.tensor,\n",
    "                                 pages_and_chunks: list[dict]=pages_and_chunks,\n",
    "                                 n_resources_to_return: int=5):\n",
    "    \"\"\"\n",
    "    Takes a query, retrieves most relevant resources and prints them out in descending order.\n",
    "\n",
    "    Note: Requires pages_and_chunks to be formatted in a specific way (see above for reference).\n",
    "    \"\"\"\n",
    "    \n",
    "    scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                                  embeddings=embeddings,\n",
    "                                                  n_resources_to_return=n_resources_to_return)\n",
    "    \n",
    "    print(f\"Query: {query}\\n\")\n",
    "    print(\"Results:\")\n",
    "    for score, index in zip(scores, indices):\n",
    "        print(f\"Score: {score:.4f}\")\n",
    "        print_wrapped(pages_and_chunks[index][\"sentence_chunk\"])\n",
    "        print(f\"Page number: {pages_and_chunks[index]['page_number']}\")\n",
    "        print(\"\\n\")\n",
    "query = \"Reinforcement Learning\"\n",
    "\n",
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                              embeddings=embeddings)\n",
    "scores, indices\n",
    "print_top_results_and_scores(query=query,\n",
    "                             embeddings=embeddings)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Harsh0910/my_merged_model\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Harsh0910/my_merged_model\")\n",
    "def get_model_num_params(model: torch.nn.Module):\n",
    "    return sum([param.numel() for param in model.parameters()])\n",
    "\n",
    "get_model_num_params(model)\n",
    "def get_model_mem_size(model: torch.nn.Module):\n",
    "    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])\n",
    "    mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
    "\n",
    "    model_mem_bytes = mem_params + mem_buffers \n",
    "    model_mem_mb = model_mem_bytes / (1024**2) \n",
    "    model_mem_gb = model_mem_bytes / (1024**3) \n",
    "\n",
    "    return {\"model_mem_bytes\": model_mem_bytes,\n",
    "            \"model_mem_mb\": round(model_mem_mb, 2),\n",
    "            \"model_mem_gb\": round(model_mem_gb, 2)}\n",
    "\n",
    "get_model_mem_size(model)\n",
    "input_text = \"What are steps of Reinforcement Learning?\"\n",
    "\n",
    "print(f\"Input text:\\n{input_text}\")\n",
    "\n",
    "dialogue_template = [\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": input_text}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                       tokenize=False,\n",
    "                                       add_generation_prompt=True)\n",
    "print(f\"\\nPrompt (formatted):\\n{prompt}\")\n",
    "print(prompt)\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\")\n",
    "print(f\"Model input (tokenized):\\n{input_ids}\\n\")\n",
    "\n",
    "outputs = model.generate(**input_ids,\n",
    "                             max_new_tokens=256) \n",
    "print(f\"Model output (tokens):\\n{outputs[0]}\\n\")\n",
    "outputs_decoded = tokenizer.decode(outputs[0])\n",
    "print(f\"Model output (decoded):\\n{outputs_decoded}\\n\")\n",
    "print(f\"Input text: {input_text}\\n\")\n",
    "print(f\"Output text:\\n{outputs_decoded.replace(prompt, '').replace('<bos>', '').replace('<eos>', '')}\")\n",
    "gpt4_questions = [\n",
    "    \"What are the primary types of machine learning, and how do they differ in approach and application?\",\n",
    "    \"How does natural language processing (NLP) enable machines to understand and generate human language?\",\n",
    "    \"Describe the architecture of a neural network and its role in deep learning.\",\n",
    "    \"What is reinforcement learning, and how is it applied in real-world scenarios?\",\n",
    "    \"Explain the importance of data preprocessing in machine learning pipelines.\"\n",
    "]\n",
    "\n",
    "manual_questions = [\n",
    "    \"What is the Turing Test, and why is it significant in AI development?\",\n",
    "    \"What are the steps of the backpropagation algorithm in training neural networks?\",\n",
    "    \"How is overfitting detected, and what are strategies to mitigate it?\",\n",
    "    \"What are the differences between precision, recall, and F1-score in evaluating model performance?\",\n",
    "    \"Name three popular frameworks for building AI models and their primary features.\"\n",
    "]\n",
    "query_list = gpt4_questions + manual_questions\n",
    "import random\n",
    "query = random.choice(query_list)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                              embeddings=embeddings)\n",
    "scores, indices\n",
    "def prompt_formatter(query: str, \n",
    "                     context_items: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Augments query with text-based context from context_items.\n",
    "    \"\"\"\n",
    "    context = \"- \" + \"\\n- \".join([item[\"sentence_chunk\"] for item in context_items])\n",
    "    base_prompt = \"\"\"Based on the following context items, please answer the query.\n",
    "Give yourself room to think by extracting relevant passages from the context before answering the query.\n",
    "Don't return the thinking, only return the answer.\n",
    "Make sure your answers are as explanatory as possible.\n",
    "Use the following examples as reference for the ideal answer style.\n",
    "\\nExample 1:\n",
    "Query: What are the types of machine learning?\n",
    "Answer: Machine learning can be categorized into three primary types: supervised learning, unsupervised learning, and reinforcement learning. Supervised learning involves training a model on labeled data, where the correct answers are provided, enabling the model to predict outcomes for new data. Unsupervised learning deals with unlabeled data, where the model identifies patterns or groupings without explicit guidance, such as clustering or dimensionality reduction. Reinforcement learning is based on training agents to make decisions by rewarding desirable behaviors and penalizing undesired ones, making it suitable for dynamic and interactive tasks like gaming and robotics.\n",
    "\\nExample 2:\n",
    "Query: How do neural networks work?\n",
    "Answer: Neural networks are computational models inspired by the human brain. They consist of layers of interconnected nodes, or neurons, which process data by assigning weights to inputs and applying an activation function. Data flows through input layers, hidden layers, and an output layer, where each connection's weight is adjusted during training using optimization techniques like backpropagation. This allows the network to learn complex patterns and relationships in the data, enabling tasks such as image recognition, natural language processing, and predictive analytics.\n",
    "\\nExample 3:\n",
    "Query: What is transfer learning in AI?\n",
    "Answer: Transfer learning in AI is a technique where a pre-trained model developed for one task is adapted to a different but related task. This approach leverages the knowledge already learned by the model, reducing the amount of data and computational resources needed for the new task. For instance, a neural network trained to recognize objects in images can be fine-tuned to identify specific types of objects, such as animals or vehicles. Transfer learning is especially valuable when working with limited labeled data for the target task, as it accelerates model development and improves accuracy.\n",
    "\\nNow use the following context items to answer the user query:\n",
    "{context}\n",
    "\\nRelevant passages: <extract relevant passages from the context here>\n",
    "User query: {query}\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "    base_prompt = base_prompt.format(context=context, query=query)\n",
    "    dialogue_template = [\n",
    "        {\"role\": \"user\",\n",
    "        \"content\": base_prompt}\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                          tokenize=False,\n",
    "                                          add_generation_prompt=True)\n",
    "    return prompt\n",
    "query = random.choice(query_list)\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                              embeddings=embeddings)\n",
    "    \n",
    "context_items = [pages_and_chunks[i] for i in indices]\n",
    "\n",
    "prompt = prompt_formatter(query=query,\n",
    "                          context_items=context_items)\n",
    "print(prompt)\n",
    "%%time\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**input_ids,\n",
    "                             temperature=0.7, \n",
    "                             do_sample=True, \n",
    "                             max_new_tokens=256)\n",
    "\n",
    "output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"RAG answer:\\n{output_text.replace(prompt, '')}\")\n",
    "def ask(query, \n",
    "        temperature=0.7,\n",
    "        max_new_tokens=512,\n",
    "        format_answer_text=True, \n",
    "        return_answer_only=True):\n",
    "    \"\"\"\n",
    "    Takes a query, finds relevant resources/context and generates an answer to the query based on the relevant resources.\n",
    "    \"\"\"\n",
    "    \n",
    "    scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                                  embeddings=embeddings)\n",
    "    \n",
    "    context_items = [pages_and_chunks[i] for i in indices]\n",
    "\n",
    "    for i, item in enumerate(context_items):\n",
    "        item[\"score\"] = scores[i].cpu() \n",
    "        \n",
    "    prompt = prompt_formatter(query=query,\n",
    "                              context_items=context_items)\n",
    "    \n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = model.generate(**input_ids,\n",
    "                                 temperature=temperature,\n",
    "                                 do_sample=True,\n",
    "                                 max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "    if format_answer_text:\n",
    "        output_text = output_text.replace(prompt, \"\").replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").replace(\"Sure, here is the answer to the user query:\\n\\n\", \"\")\n",
    "\n",
    "    if return_answer_only:\n",
    "        return output_text\n",
    "    \n",
    "    return output_text, context_items\n",
    "query = random.choice(query_list)\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "answer, context_items = ask(query=query, \n",
    "                            temperature=0.7,\n",
    "                            max_new_tokens=512,\n",
    "                            return_answer_only=False)\n",
    "\n",
    "print(f\"Answer:\\n\")\n",
    "print_wrapped(answer)\n",
    "print(f\"Context items:\")\n",
    "context_items\n",
    "from huggingface_hub import login\n",
    "login()\n",
    "import pickle\n",
    "\n",
    "session_data = {\n",
    "    \"embeddings\": embeddings.cpu(),  \n",
    "    \"pages_and_chunks\": pages_and_chunks,\n",
    "    \"query\": query,\n",
    "    \"answer\": answer,\n",
    "    \"context_items\": context_items\n",
    "}\n",
    "\n",
    "with open(\"session_data.pkl\", \"wb\") as session_file:\n",
    "    pickle.dump(session_data, session_file)\n",
    "\n",
    "print(\"Session saved to 'session_data.pkl'\")\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "repo_name = \"my-session-data\"  \n",
    "repo_url = api.create_repo(repo_name, exist_ok=True)\n",
    "\n",
    "print(f\"Repository created: {repo_url}\")\n",
    "\n",
    "from huggingface_hub import HfApi, HfFolder\n",
    "\n",
    "repo_id = \"Harsh0910/my-session-data\"  \n",
    "file_path = \"session_data.pkl\"  \n",
    "\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=file_path,\n",
    "    path_in_repo=\"session_data.pkl\",  \n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\" \n",
    ")\n",
    "\n",
    "print(f\"File uploaded to Hugging Face Hub at: https://huggingface.co/{repo_id}\")\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "import pickle\n",
    "\n",
    "file_path = hf_hub_download(repo_id=\"Harsh0910/my-session-data\", filename=\"session_data.pkl\")\n",
    "\n",
    "with open(file_path, \"rb\") as session_file:\n",
    "    session_data = pickle.load(session_file)\n",
    "\n",
    "embeddings = session_data[\"embeddings\"]\n",
    "pages_and_chunks = session_data[\"pages_and_chunks\"]\n",
    "query = session_data[\"query\"]\n",
    "answer = session_data[\"answer\"]\n",
    "context_items = session_data[\"context_items\"]\n",
    "\n",
    "print(\"Session restored successfully from Hugging Face!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
